{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3]) torch.Size([3, 3])\n",
      "tensor([[ 1,  4,  9],\n",
      "        [16, 25, 36],\n",
      "        [49, 64, 81]]) torch.Size([3, 3])\n",
      "tensor([[ 30,  36,  42],\n",
      "        [ 66,  81,  96],\n",
      "        [102, 126, 150]]) torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.tensor([[1,2,3,],[4,5,6],[7,8,9]])\n",
    "m2 = torch.tensor([[1,2,3,],[4,5,6],[7,8,9]])\n",
    "\n",
    "print(m1.shape, m2.shape)\n",
    "\n",
    "res1 = m1.mul(m2) #행렬 원소곱\n",
    "res2 = m1.matmul(m2) #행렬 곱\n",
    "#행렬곱의 경우 m1@m2로 표현 가능하다\n",
    "\n",
    "print(res1, res1.shape)\n",
    "print(res2, res2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([2, 4])\n",
      "torch.Size([4, 2]) tensor([[ 50,  60],\n",
      "        [114, 140],\n",
      "        [178, 220],\n",
      "        [242, 300]])\n"
     ]
    }
   ],
   "source": [
    "# 4x2 텐서 자료형 곱하기\n",
    "m1 = torch.tensor([[1,2,3,4,],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\n",
    "\n",
    "m2 = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
    "\n",
    "print(m1.shape, m2.shape)\n",
    "\n",
    "m2 = m2.reshape(4,2)\n",
    "\n",
    "res1 = m1@m2 #행렬곱\n",
    "\n",
    "print(res1.shape, res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3]) torch.Size([3])\n",
      "tensor([71., 81., 91.]) torch.Size([3])\n",
      "tensor([[23., 27., 31.],\n",
      "        [45., 53., 61.],\n",
      "        [67., 79., 91.]]) torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# raw layer을 만들어보기\n",
    "\n",
    "W = torch.FloatTensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]]) #weight matrix\n",
    "B = torch.FloatTensor([1,1,1]) #bias matrix\n",
    "\n",
    "print(W.shape, B.shape)\n",
    "\n",
    "def linear_1(x, w, b):\n",
    "    y = torch.matmul(x, w) + b\n",
    "    return y\n",
    "\n",
    "#이제 레이어 연산 y = f(x) +b 해보기\n",
    "\n",
    "x1 = torch.FloatTensor([1,2,3,4])\n",
    "\n",
    "x2 = torch.FloatTensor([[1,1,1,1],[2,2,2,2],[3,3,3,3]])\n",
    "#[batch_size, imput_dim]차원인 x2\n",
    "\n",
    "\n",
    "layer_1 = linear_1(x1, W, B)\n",
    "layer_2 = linear_1(x2, W, B) #batch처리 -> input(x)가 여러 레이어\n",
    "\n",
    "print(layer_1, layer_1.shape)\n",
    "print(layer_2, layer_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module() -> 모든 뉴럴넷 모형의 기본이 됨\n",
    "이거로 한번 실습해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn #각 레이어의 함수, 신경망 구조를 정의할 때 사용하는 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear_1(nn.Module): #클래스 상속 후 재정의 과정\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.W = torch.FloatTensor(input_dim, output_dim)\n",
    "        self.b = torch.FloatTensor(output_dim)\n",
    "\n",
    "        print(self.W, self.b) #W랑 b는 임의 원소값으로 채워지는 듯?\n",
    "\n",
    "    def forward(self, x): #input을 받아서 레이어를 통과시키는 연산 함수\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1367e+22,  2.3703e-38,  1.4013e-45],\n",
      "        [-4.2734e+22,  2.3711e-38,  1.4013e-45],\n",
      "        [-6.4100e+22,  2.3720e-38,  1.4013e-45],\n",
      "        [ 1.3813e+03,  0.0000e+00,  1.5616e-05]]) tensor([3.5000e+01, 6.0240e-01, 2.3694e-38])\n",
      "tensor([[-1.2820e+23,  6.0240e-01,  1.5616e-05],\n",
      "        [-2.5640e+23,  6.0240e-01,  3.1233e-05],\n",
      "        [-3.8460e+23,  6.0240e-01,  4.6849e-05]])\n"
     ]
    }
   ],
   "source": [
    "x2 = torch.FloatTensor([[1,1,1,1],[2,2,2,2],[3,3,3,3]])\n",
    "#[batch_size = 3, imput_dim = 4]차원인 x2\n",
    "\n",
    "linear2 = MyLinear_1(4,3)\n",
    "\n",
    "y = linear2(x2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in linear2.parameters(): #클래스 내 파라미터를 확인하기 위한 for구문\n",
    "    print(i) #일단 이거는 아무것도 출력이 되지 않는다 -> 이유는 아래에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear_1(nn.Module): #클래스 상속 후 재정의 과정\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.W = torch.FloatTensor(input_dim, output_dim)\n",
    "        self.b = torch.FloatTensor(output_dim)\n",
    "\n",
    "    def forward(self, x): #input을 받아서 레이어를 통과시키는 연산 함수\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "\n",
    "        return y\n",
    "    \n",
    "\n",
    "class MyLinear_2(nn.Module): #위 클래서에서 파라미터값 출력을 위한 부분이 추가된 클래스\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "        #네트워크로 설계할 때 학습이 필요한 W, B리고 알려줄 때 nn.Parameter\n",
    "\n",
    "        print(self.W, self.b) #W랑 b는 임의 원소값으로 채워지는 듯?\n",
    "\n",
    "    def forward(self, x): #input을 받아서 레이어를 통과시키는 연산 함수\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-3.5609e+21,  1.4349e-42,  1.0000e+00],\n",
      "        [ 1.0000e+00,  2.0000e+00,  2.0000e+00],\n",
      "        [ 2.0000e+00,  2.0000e+00,  3.0000e+00],\n",
      "        [ 3.0000e+00,  3.0000e+00,  3.0000e+00]], requires_grad=True) Parameter containing:\n",
      "tensor([0.0000e+00, 0.0000e+00, 2.3694e-38], requires_grad=True)\n",
      "tensor([[  813.0000,  3744.0459,  1101.0000],\n",
      "        [ 1626.0000,  7488.0918,  2202.0000],\n",
      "        [ 2439.0000, 11232.1377,  3303.0000]]) tensor([[-3.5609e+21,  7.0000e+00,  9.0000e+00],\n",
      "        [-7.1218e+21,  1.4000e+01,  1.8000e+01],\n",
      "        [-1.0683e+22,  2.1000e+01,  2.7000e+01]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x2 = torch.FloatTensor([[1,1,1,1],[2,2,2,2],[3,3,3,3]])\n",
    "#[batch_size = 3, imput_dim = 4]차원인 x2\n",
    "\n",
    "linear__1 = MyLinear_1(4,3)\n",
    "linear__2 = MyLinear_2(4,3)\n",
    "\n",
    "y_1 = linear__1(x2)\n",
    "y_2 = linear__2(x2)\n",
    "print(y_1, y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-3.5609e+21,  1.4349e-42,  1.0000e+00],\n",
      "        [ 1.0000e+00,  2.0000e+00,  2.0000e+00],\n",
      "        [ 2.0000e+00,  2.0000e+00,  3.0000e+00],\n",
      "        [ 3.0000e+00,  3.0000e+00,  3.0000e+00]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0000e+00, 0.0000e+00, 2.3694e-38], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in linear__2.parameters(): #클래스 내 파라미터를 확인하기 위한 for구문\n",
    "    print(i) #이 구문은 객체 linear__2의 내부에는 파라미터를 확인할 수 있도록 구문이 포함되어 있어서\n",
    "             #중간의 파라미터값이 출력된다. (이 파라미터는 W, B 두개를 출력한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear() -> 위 과정을 한큐에 다 해준 클래스가 있는데 그게 nn.Linear()클래스임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3835e-01, -6.6569e-01, -2.1696e-01],\n",
      "        [-7.0271e-02, -1.7949e+00, -6.3066e-01],\n",
      "        [-2.1936e-03, -2.9241e+00, -1.0444e+00]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([3, 3])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0188,  0.2644, -0.1616, -0.0535],\n",
      "        [-0.2043, -0.2803, -0.2059, -0.4387],\n",
      "        [-0.2059,  0.0281,  0.1152, -0.3511]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2064,  0.4635,  0.1967], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0188,  0.2644, -0.1616, -0.0535],\n",
      "        [-0.2043, -0.2803, -0.2059, -0.4387],\n",
      "        [-0.2059,  0.0281,  0.1152, -0.3511]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2064,  0.4635,  0.1967], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x2 = torch.FloatTensor([[1,1,1,1],[2,2,2,2],[3,3,3,3]])\n",
    "#[batch_size = 3, imput_dim = 4]차원인 x2\n",
    "\n",
    "linear_3 = nn.Linear(in_features=4, out_features=3)\n",
    "\n",
    "y_3 = linear_3(x2)\n",
    "print(y_3)\n",
    "print(y_3.shape)\n",
    "\n",
    "for p in linear_3.parameters():\n",
    "    print(p)\n",
    "\n",
    "print(linear_3.weight)\n",
    "print(linear_3.bias)\n",
    "\n",
    "#nn.Linear클래스는 일반적인 Fully Connected Layer이 다 설계되어 있는 클래스이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#앞으로 Fullay Connected Layer 클래스의 설계는 아래와 같이 수행한다.\n",
    "\n",
    "class MyLinear_3(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super.__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "        #self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "        #self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "        #이 구문을 한번에 줄여준게 nn.Linear\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Activation Function </br>\n",
    "\n",
    "활성화 함수에 관한 클래스가 따로 정의되어 있으니 그것을 import하면 된다</br>\n",
    "활성화 함수는 relu, sigmod, tanh, leeky_relu, softmax등을 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F #이게 활성화 함수를 제공하는 클래스(메서드함수군)\n",
    "#해당 모듈에는 relu, sigmod, tanh, leeky_relu, softmax등이 포함되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8714,  0.2551,  0.1086],\n",
      "        [-0.6204,  0.9494,  0.8667]]) tensor([[0.2950, 0.5634, 0.5271],\n",
      "        [0.3497, 0.7210, 0.7041]])\n"
     ]
    }
   ],
   "source": [
    "#sigmod함수 예제\n",
    "ex_sigmod = nn.Sigmoid()\n",
    "input = torch.randn(2,3) #임의의 input 매트릭스 생성\n",
    "\n",
    "output = ex_sigmod(input)\n",
    "print(input, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1468,  1.4263, -0.1081],\n",
      "        [-0.4708, -0.8003,  0.1103]]) tensor([[0.1468, 1.4263, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1103]])\n"
     ]
    }
   ],
   "source": [
    "#Relu함수 예제\n",
    "ex_relu = nn.ReLU()\n",
    "input = torch.randn(2,3) #임의의 input 매트릭스 생성\n",
    "\n",
    "output = ex_relu(input)\n",
    "print(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7705, -0.3533,  0.6807],\n",
      "        [ 0.4040, -0.3561,  0.4958]]) tensor([[0.4466, 0.1452, 0.4082],\n",
      "        [0.3901, 0.1824, 0.4275]])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#softmax함수 예제 -> 이 함수는 dim을 지정해줘야 한다.(왠만하면 dim=1을 쓰는 듯?)\n",
    "ex_softmax = nn.Softmax(dim=1)\n",
    "input = torch.randn(2,3) #임의의 input 매트릭스 생성\n",
    "\n",
    "output = ex_softmax(input)\n",
    "print(input, output)\n",
    "print(output[0].sum()) #이 결과값은 1(dim값)이 나온다.\n",
    "print(output[1].sum()) #이 결과값은 1(dim값)이 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5087)\n"
     ]
    }
   ],
   "source": [
    "#첫번째 MSE loss function -> 이 함수는 주로 Regression Task에 사용됨\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "pred = torch.randn(4, 10)\n",
    "output = torch.rand(4, 10)\n",
    "#임의의 두 매트릭스를 만든 뒤 이를 MSELoss가 객체화된 mse_loss로 차이를 알아봄\n",
    "\n",
    "print(mse_loss(pred, output)) # 이 값이 작아지도록 하는게 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.NLLLoss() : Navigative Log Likehood Loss </br>\n",
    "이 함수는 Cross Entropy Loss 함수와 거의 동일하게 동작하는 Loss 함수라 보면 된다.</br>\n",
    "좀더 정확하게는 Softmax + Log함수인 LogSoftmax 함수랑 같이 조합해서 CEL과 유사하게 동작하게 만듬</br>\n",
    "이렇게 복잡하게 쓰는 이유 : NaN오류를 방지함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1149, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#nn.NLLLoss() 함수 실습 -> 이거로 CEL을 구현해보자\n",
    "\n",
    "m = nn.LogSoftmax(dim=1) #softmax함수의 NaN오류를 방지하는 함수\n",
    "\n",
    "nll_loss = nn.NLLLoss()\n",
    "\n",
    "pred = torch.randn([1,5], requires_grad=True)\n",
    "#이것은 [0~1, 0~1, 0~1, 0~1, 0~1]임\n",
    "\n",
    "output = torch.tensor([1]) #[0~4]사이의 클래스 라벨을 갖는다 설정하자\n",
    "#이것의 으미가 [0, 1, 0, 0, 0]임\n",
    "#즉 pred랑 output의 비교는 2번째 라벨이 된 확률은???을 물어보는 거임\n",
    "\n",
    "\n",
    "print(torch.log(F.softmax(pred))) #이렇게 softmax를 바로 쓰면 NaN이 뜰 수 잇음\n",
    "#m = nn.LogSoftmax(dim=1) == log(F.softmax(pred)) 같은 효과이나 좌변이 더 안정적\n",
    "\n",
    "cal = m(pred) #이것이 torch.log(F.softmax(pred))와 같은 뜻임\n",
    "\n",
    "print(nll_loss(cal, output)) #이렇게 써야 CEL을 구현한거\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5221, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#CEL과정 한번 더 실습\n",
    "\n",
    "pred = torch.rand([3,5], requires_grad=True)\n",
    "#위에거랑 다르게 batch size가 3개인거 (사진이 3장임)\n",
    "\n",
    "output = torch.tensor([1,0,3])\n",
    "#batch_size가 3이니, 각각 비교하는데 2번째, 1번째, 4번째 라벨이 정답(1)이라는 뜻\n",
    "\n",
    "\n",
    "cal= m(pred)\n",
    "\n",
    "print(nll_loss(cal, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7511, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#위 과정이 복잡해 -> 한번에 다 구현한 메서드\n",
    "#nn.CrossEntropyLoss() -> softmax, Nevigative Log Likelyhood 합본\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "pred = torch.randn([1,5], requires_grad=True)\n",
    "output = torch.tensor([3]) #4번째 레이블이 정답\n",
    "\n",
    "ce_loss(pred, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1128, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "pred = torch.randn([3,5], requires_grad=True)\n",
    "output = torch.tensor([1,2,3]) #4번째 레이블이 정답\n",
    "\n",
    "ce_loss(pred, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8338, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "pred = torch.randn([3,5], requires_grad=True)\n",
    "output = torch.randn([3,5]).softmax(dim=1) #이거는 라벨이 01000이 아니라 확률일 경우\n",
    "\n",
    "ce_loss(pred, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Loss의 최적화 -> GD, 백프로 파게이션</br>\n",
    "이 최적화 방법론은 여러개 있으니 숙지할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#최적화 방법론까지 구현되어 있는 모델(클래스)를 만들어보자\n",
    "\n",
    "\n",
    "class My_linear_5(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "model = My_linear_5(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1141,  0.3681,  0.2716,  0.1136,  0.0049],\n",
      "        [-0.0169,  0.2013, -0.0024, -0.0052, -0.1424],\n",
      "        [ 0.1414,  0.0012,  0.1128, -0.0920, -0.0187]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3347, -0.1129,  0.3575], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p) #업데이트 대상이 되는 W, B가 어떻게 생겻는지 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD(Stocastic Gradient Descent)방법\n",
    "\n",
    "sgd_optimizer = torch.optim.SGD(model.parameters(), lr =0.01, momentum=0.9)\n",
    "#사용되는 argument 값은 의미를 확인하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADAM 방법\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 GD방법론은 소개만 한거고 이제 본격적으로 학습에 사용하는 건 다음시간에"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
