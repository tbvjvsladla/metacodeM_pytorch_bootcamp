{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, **kwargs):\n",
    "        super(BasicConv, self).__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, bias=False, **kwargs),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU6(inplace=True) #Relu랑 같은데 상한이 6으로 제한된 레이어\n",
    "            # 기존 Relu보다 고정 소수점 연산(fixed-point arithmetic)에 더 유리함\n",
    "            # 따라서 모바일 및 임베디드 디바이스에 대하여 유리함\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthSep(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super(DepthSep, self).__init__()\n",
    "\n",
    "        self.depthwise = BasicConv(in_ch, in_ch, kernel_size=3, stride=stride, padding=1,\n",
    "                                   groups = in_ch)\n",
    "        # 여기서 groups 입력 채널과 출력 채널사이의 관계를 나타냄\n",
    "        # default=1 => 모든 입력은 모든 출력과 conv 연산이 됨\n",
    "        # 2, 3, 4 => 입력을 2, 3, 4 그룹으로 나누어서 각각 conv연산 후 concat\n",
    "        # group = in_ch --> 이게 Depthwise의 `Separable`에 해당하는 항목임\n",
    "\n",
    "        self.pointwise = BasicConv(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV1(nn.Module):\n",
    "    def __init__(self, width_multiplier, num_classes=1000, init_weight=True):\n",
    "        super(MobileNetV1, self).__init__()\n",
    "\n",
    "        self.alpha = width_multiplier #네트워크 각 층의 필터 개수를 조정하는 인자값\n",
    "\n",
    "        self.stem = BasicConv(3, int(32*self.alpha), kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.feature_ext = nn.Sequential(\n",
    "            DepthSep(int(32*self.alpha), int(64*self.alpha)),\n",
    "            DepthSep(int(64*self.alpha), int(128*self.alpha), stride=2),\n",
    "            DepthSep(int(128*self.alpha), int(128*self.alpha)),\n",
    "            DepthSep(int(128*self.alpha), int(256*self.alpha), stride=2),\n",
    "            DepthSep(int(256*self.alpha), int(256*self.alpha)),\n",
    "            DepthSep(int(256*self.alpha), int(512*self.alpha), stride=2),\n",
    "            DepthSep(int(512*self.alpha), int(512*self.alpha)),\n",
    "            DepthSep(int(512*self.alpha), int(512*self.alpha)),\n",
    "            DepthSep(int(512*self.alpha), int(512*self.alpha)),\n",
    "            DepthSep(int(512*self.alpha), int(512*self.alpha)),\n",
    "            DepthSep(int(512*self.alpha), int(512*self.alpha)),\n",
    "            DepthSep(int(512*self.alpha), int(1024*self.alpha), stride=2),\n",
    "            DepthSep(int(1024*self.alpha), int(1024*self.alpha))\n",
    "        )\n",
    "\n",
    "        self.classfier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(int(1024*self.alpha), num_classes)\n",
    "        )\n",
    "\n",
    "        if init_weight: #초기화 구동함수 호출\n",
    "            self._initialize_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.feature_ext(x)\n",
    "        x = self.classfier(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    #모델의 초기 Random을 커스터마이징 하기 위한 함수\n",
    "    def _initialize_weight(self):\n",
    "        for m in self.modules(): #설계한 모델의 모든 레이어를 순회\n",
    "            if isinstance(m, nn.Conv2d): #conv의 파라미터(weight, bias)의 초가깂설정\n",
    "                # Kaiming 초기화를 사용한 이유:\n",
    "                # Kaiming 초기화는 ReLU 활성화 함수와 함께 사용될 때 좋은 성능을 보임\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "            elif isinstance(m, nn.BatchNorm2d): #BN의 파라미터(weight, bias)의 초가깂설정\n",
    "                # BatchNorm 레이어의 가중치와 바이어스를 간단한 값으로 초기화\n",
    "                nn.init.constant_(m.weight, 1) # 1로 다 채움\n",
    "                nn.init.constant_(m.bias, 0) # 0으로 다 채움\n",
    "\n",
    "            elif isinstance(m, nn.Linear): #FCL의 파라미터(weight, bias)의 초기값 설정\n",
    "                # 선형 레이어의 가중치를 정규 분포로 초기화\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_keys = ['W_100%', 'W_75%', 'W_50%'] #모델에 Width Multiplier\n",
    "d_keys = ['R_224', 'R_192', 'R_128'] #데이터셋에 Resolution Multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "model_100 = MobileNetV1(width_multiplier=1.0, num_classes=10)\n",
    "model_75 = MobileNetV1(width_multiplier=0.75, num_classes=10)\n",
    "model_50 = MobileNetV1(width_multiplier=0.5, num_classes=10)\n",
    "\n",
    "models = {\n",
    "    'W_100%' : model_100.to('cpu'),\n",
    "    'W_75%' : model_75.to('cpu'),\n",
    "    'W_50%' : model_50.to('cpu'),\n",
    "}\n",
    "\n",
    "# 모델의 초기 가중치 저장\n",
    "initial_weights = {\n",
    "    'W_100%': copy.deepcopy(model_100.state_dict()),\n",
    "    'W_75%': copy.deepcopy(model_75.state_dict()),\n",
    "    'W_50%': copy.deepcopy(model_50.state_dict())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets\n",
    "\n",
    "root = './data/Animals-10'\n",
    "\n",
    "# 원래 img_dataset 초기화\n",
    "original_img_dataset = {}\n",
    "\n",
    "original_img_dataset['train'] = datasets.ImageFolder(os.path.join(root, 'train'))\n",
    "original_img_dataset['val'] = datasets.ImageFolder(os.path.join(root, 'val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 방법론 정의\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "animals_val = {'mean' : [0.5177, 0.5003, 0.4126],\n",
    "                'std' : [0.2133, 0.2130, 0.2149]\n",
    "}\n",
    "\n",
    "def define_transform(img_size, normal_val, augment=False):\n",
    "    transform_list = []\n",
    "\n",
    "    if augment:\n",
    "        transform_list += [ #데이터 증강은 반전, 색상밝기채도, 아핀 3가지\n",
    "            v2.RandomHorizontalFlip(p=0.5),\n",
    "            v2.ColorJitter(brightness=0.4,\n",
    "                            contrast=0.4,\n",
    "                            saturation=0.4,\n",
    "                            hue=0.1),\n",
    "            v2.RandomAffine(degrees=(30, 70),\n",
    "                            translate=(0.1, 0.3),\n",
    "                            scale=(0.5, 0.75)),\n",
    "        ]\n",
    "\n",
    "    transform_list += [\n",
    "        v2.Resize((img_size, img_size)), #이미지 사이즈별로 리사이징\n",
    "        v2.ToImage(),  #이미지를 Tensor 자료형으로 변환\n",
    "        v2.ToDtype(torch.float32, scale=True), #텐서 자료형을 [0~1]로 정규화\n",
    "        v2.Normalize(mean=normal_val['mean'], std=normal_val['std']) #데이터셋 표준화\n",
    "    ]\n",
    "\n",
    "    return v2.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transfroms = {\n",
    "    'R_224' : define_transform(224, animals_val, augment=True),\n",
    "    'R_192' : define_transform(192, animals_val, augment=True),\n",
    "    'R_128' : define_transform(128, animals_val, augment=True)\n",
    "}\n",
    "\n",
    "val_transfroms = {\n",
    "    'R_224' : define_transform(224, animals_val, augment=False),\n",
    "    'R_192' : define_transform(192, animals_val, augment=False),\n",
    "    'R_128' : define_transform(128, animals_val, augment=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_dataset 딥카피\n",
    "img_dataset = {\n",
    "    'R_224': copy.deepcopy(original_img_dataset),\n",
    "    'R_192': copy.deepcopy(original_img_dataset),\n",
    "    'R_128': copy.deepcopy(original_img_dataset),\n",
    "}\n",
    "\n",
    "for d_key in img_dataset:\n",
    "    img_dataset[d_key]['train'].transform = train_transfroms[d_key]\n",
    "    img_dataset[d_key]['val'].transform = val_transfroms[d_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bs = 256 #배치사이즈 크기\n",
    "\n",
    "#데이터로더 생성\n",
    "dataloaders_dict = {\n",
    "    'R_224': None,\n",
    "    'R_192': None,\n",
    "    'R_128': None,\n",
    "}\n",
    "\n",
    "for d_key in dataloaders_dict:\n",
    "    dataloaders_dict[d_key] = {\n",
    "        'train' : DataLoader(img_dataset[d_key]['train'], batch_size=bs, shuffle=True),\n",
    "        'val' : DataLoader(img_dataset[d_key]['val'], batch_size=bs, shuffle=False),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU사용 가능여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# 모델을 GPU로 이전\n",
    "for m_key in models:\n",
    "    models[m_key].to(device)\n",
    "\n",
    "#손실함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizers = {\n",
    "    'W_100%': optim.Adam(models['W_100%'].parameters(), lr=0.001),\n",
    "    'W_75%': optim.Adam(models['W_75%'].parameters(), lr=0.001),\n",
    "    'W_50%': optim.Adam(models['W_50%'].parameters(), lr=0.001)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전에 모듈화 한 학습/검증용 라이브러리 import\n",
    "from C_ModelTrainer import ModelTrainer\n",
    "\n",
    "epoch_step = 3 #특정 epoch마다 모델의 훈련/검증 정보 출력\n",
    "# BC_mode = True : 이진분류 문제 풀이 , BC_mode = False : 다중분류 문제 풀이\n",
    "trainer = ModelTrainer(epoch_step=epoch_step, device=device.type, BC_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습과 검증 손실 및 정확도를 저장할 딕셔너리\n",
    "d_keys = dataloaders_dict.keys()\n",
    "m_keys = models.keys()\n",
    "history = {mk: {dk: {'loss': [], 'accuracy': []} for dk in d_keys} for mk in m_keys}\n",
    "\n",
    "num_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[훈련중]로스: 2.114, 정확도: 0.229: 100%|██████████| 87/87 [04:40<00:00,  3.22s/it]\n",
      "100%|██████████| 16/16 [00:11<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001,\t[W_100%, R_224] 훈련 [Loss: 2.114, Acc: 22.87%]\n",
      "epoch 001,\t[W_100%, R_224] 검증 [Loss: 2.900, Acc: 19.10%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[훈련중]로스: 2.166, 정확도: 0.211: 100%|██████████| 87/87 [04:35<00:00,  3.16s/it]\n",
      "100%|██████████| 16/16 [00:10<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001,\t[W_75%, R_224] 훈련 [Loss: 2.166, Acc: 21.09%]\n",
      "epoch 001,\t[W_75%, R_224] 검증 [Loss: 2.434, Acc: 27.00%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[훈련중]로스: 2.148, 정확도: 0.216: 100%|██████████| 87/87 [04:33<00:00,  3.14s/it]\n",
      "100%|██████████| 16/16 [00:10<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001,\t[W_50%, R_224] 훈련 [Loss: 2.148, Acc: 21.65%]\n",
      "epoch 001,\t[W_50%, R_224] 검증 [Loss: 2.663, Acc: 23.92%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[훈련중]로스: 2.178, 정확도: 0.202: 100%|██████████| 87/87 [04:24<00:00,  3.05s/it]\n",
      "100%|██████████| 16/16 [00:09<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001,\t[W_100%, R_192] 훈련 [Loss: 2.178, Acc: 20.21%]\n",
      "epoch 001,\t[W_100%, R_192] 검증 [Loss: 2.392, Acc: 20.93%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[훈련중]로스: 2.183, 정확도: 0.197: 100%|██████████| 87/87 [04:22<00:00,  3.02s/it]\n",
      "100%|██████████| 16/16 [00:09<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001,\t[W_75%, R_192] 훈련 [Loss: 2.183, Acc: 19.67%]\n",
      "epoch 001,\t[W_75%, R_192] 검증 [Loss: 2.344, Acc: 17.13%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[훈련중]로스: 2.162, 정확도: 0.207: 100%|██████████| 87/87 [04:17<00:00,  2.96s/it]\n",
      "100%|██████████| 16/16 [00:09<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001,\t[W_50%, R_192] 훈련 [Loss: 2.162, Acc: 20.70%]\n",
      "epoch 001,\t[W_50%, R_192] 검증 [Loss: 2.234, Acc: 23.84%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[훈련중]로스: 2.170, 정확도: 0.210: 100%|██████████| 87/87 [04:04<00:00,  2.81s/it]\n",
      "100%|██████████| 16/16 [00:06<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001,\t[W_100%, R_128] 훈련 [Loss: 2.170, Acc: 21.04%]\n",
      "epoch 001,\t[W_100%, R_128] 검증 [Loss: 2.299, Acc: 21.75%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[훈련중]로스: 2.191, 정확도: 0.199: 100%|██████████| 87/87 [04:04<00:00,  2.81s/it]\n",
      "100%|██████████| 16/16 [00:06<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001,\t[W_75%, R_128] 훈련 [Loss: 2.191, Acc: 19.93%]\n",
      "epoch 001,\t[W_75%, R_128] 검증 [Loss: 2.246, Acc: 20.27%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[훈련중]로스: 2.187, 정확도: 0.195: 100%|██████████| 87/87 [04:02<00:00,  2.79s/it]\n",
      "100%|██████████| 16/16 [00:06<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001,\t[W_50%, R_128] 훈련 [Loss: 2.187, Acc: 19.50%]\n",
      "epoch 001,\t[W_50%, R_128] 검증 [Loss: 2.165, Acc: 21.88%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련 및 평가\n",
    "for d_key in dataloaders_dict:\n",
    "    dataloaders = dataloaders_dict[d_key]\n",
    "    for m_key in models:\n",
    "        model = models[m_key]\n",
    "        optimizer = optimizers[m_key]\n",
    "\n",
    "        #학습/검증 epoch 수행 전 모델 파라미터 초기화\n",
    "        model.load_state_dict(initial_weights[m_key])\n",
    "\n",
    "        for epoch in range(num_epoch):\n",
    "            # 훈련 손실과 훈련 성과지표를 반환 받습니다.\n",
    "            train_loss, train_acc = trainer.model_train(model, dataloaders['train'], \n",
    "                                                        criterion, optimizer, epoch)\n",
    "\n",
    "            # 검증 손실과 검증 성과지표를 반환 받습니다.\n",
    "            test_loss, test_acc = trainer.model_evaluate(model, dataloaders['val'], \n",
    "                                                         criterion, epoch)\n",
    "\n",
    "            # 손실과 성능지표를 리스트에 저장\n",
    "            history[m_key][d_key]['loss'].append((train_loss, test_loss))\n",
    "            history[m_key][d_key]['accuracy'].append((train_acc, test_acc))\n",
    "\n",
    "            # epoch가 특정 배수일 때만 출력하기\n",
    "            if (epoch + 1) % epoch_step == 0 or epoch == 0:\n",
    "                print(f\"epoch {epoch+1:03d},\" + \"\\t\" + \n",
    "                      f\"[{m_key}, {d_key}] 훈련 [Loss: {train_loss:.3f}, \" +\n",
    "                      f\"Acc: {train_acc*100:.2f}%]\")\n",
    "                print(f\"epoch {epoch+1:03d},\" + \"\\t\" + \n",
    "                      f\"[{m_key}, {d_key}] 검증 [Loss: {test_loss:.3f}, \" +\n",
    "                      f\"Acc: {test_acc*100:.2f}%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU6'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[W_100%, R_224] FLOPs: 587.949M, Params: 3.217M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU6'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[W_100%, R_192] FLOPs: 431.965M, Params: 3.217M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU6'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[W_100%, R_128] FLOPs: 191.991M, Params: 3.217M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU6'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[W_75%, R_224] FLOPs: 339.807M, Params: 1.824M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU6'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[W_75%, R_192] FLOPs: 249.656M, Params: 1.824M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU6'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[W_75%, R_128] FLOPs: 110.963M, Params: 1.824M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU6'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[W_50%, R_224] FLOPs: 159.101M, Params: 823.722K\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU6'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[W_50%, R_192] FLOPs: 116.892M, Params: 823.722K\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU6'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[W_50%, R_128] FLOPs: 51.955M, Params: 823.722K\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "input_tensors = {\n",
    "    'R_224' : torch.randn(1, 3, 224, 224).to(device),\n",
    "    'R_192' : torch.randn(1, 3, 192, 192).to(device),\n",
    "    'R_128' : torch.randn(1, 3, 128, 128).to(device),\n",
    "}\n",
    "\n",
    "for d_key in models:\n",
    "    for m_key in input_tensors:\n",
    "        # FLOPs 및 파라미터 수 계산 -> 여기서 inputs는 `튜플` 자료형이어야만 한다!!\n",
    "        flops, params = profile(models[d_key], inputs=(input_tensors[m_key], ))\n",
    "\n",
    "        # 보기 좋은 형식으로 출력\n",
    "        flops, params = clever_format([flops, params], \"%.3f\")\n",
    "        print(f\"[{d_key}, {m_key}] FLOPs: {flops}, Params: {params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
